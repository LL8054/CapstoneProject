class(vowel.train)
colnames(vowel.train)
class(vowel.train$y)
vowel.train$y <- as.factor(vowel.train$y)
vowel.test$y <- as.factor(vowel.test$y)
class(vowel.test$y)
set.seed(33833)
modelFit <- train(y~., vowel.train, method="rf")
order(varImp(modelFit))
set.seed(33833)
modelFit <- train(y~., vowel.train, method="rf", prox=TRUE)
order(varImp(modelFit))
modelFit
varImp(modelFit)
order(varImp(modelFit))
class(modelFit)
a <- randomForest(y ~ ., data = vowel.train, importance = FALSE)
varImp(a)
order(varImp(a))
vowel.train$y <- as.factor(vowel.train$y)
vowel.test$y <- as.factor(vowel.test$y)
set.seed(33833)
modelFit <- train(y~., vowel.train, method="rf", prox=TRUE)
varImp(modelFit)
prediction <- predict(vowel.test, modelFit)
prediction <- predict(modelFit, vowel.test)
varImp(prediction)
prediction
modelFit
prediction
vowel.train$y <- as.factor(vowel.train$y)
vowel.test$y <- as.factor(vowel.test$y)
set.seed(3833)
modelFit <- train(y~., vowel.train, method="rf", prox=TRUE)
varImp(modelFit)
library(ISLR)
install.packages("ISLR")
library(ISLR)
data(Wage)
library(ggplot2)
library(caret)
inBuild <- createDataPartition(y=Wage$wage, p=.7, list=FALSE)
validation <- Wage[-inBuild,]
buildData <- Wage[inBuild]
inTrain <- createDataPartition(y=buildData$wage, p=.7, list=FALSE)
buildData <- Wage[inBuild,]
inTrain <- createDataPartition(y=buildData$wage, p=.7, list=FALSE)
training <- buildData[inTrain,]
testing <- buildData[-inTrain,]
dim(training)
dim(testing)
dim(validatin)
dim(validation)
mod1 <- train(wage ~ ., method="glm", data=training)
mod2 <- train(wage ~., method="rf", data=training, trControl=trainControl(method="cv"), number=3)
pred1 <- predict(mod1, testing)
pred1
pred2 <- predict(mod2, testing)
qplot(pred1, pred2, color=wage, data=testing)
testing$wage
training$wage
preDF <- data.frame(pred1, pred2, wage=testing$wage)
preDF
head(preDF)
exit
a <- c(4, 0, 4, 0, 3, 0, 10, 1, 6, 4, 3, 8, 2, 3, 1, 10, 7, 5, 2, 1, 5, 1, 1, 2)
plot(a)
hist(a)
hist(a, breaks=10)
hist(a, breaks=12)
hist(a, breaks=c(0:10))
hist(a, breaks=c(0:11))
hist(a, breaks=c(0:10))
mean(a)
abline(v = mean(a), col = "blue", lwd = 2)
hist(a, breaks=10, main="Rangers Runs Per Game in April"
)
hist(a, breaks=10, main="Rangers Runs Per Game in April", x.lab="Runs Per Game")
hist(a, breaks=10, main="Rangers Runs Per Game in April", xlab="Runs Per Game")
abline(v = mean(a), col = "blue", lwd = 2)
setwd("~/datasciencecoursera/Courses/Capstone/capstoneproject")
path <- "~/datasciencecoursera/Courses/Capstone/Coursera-SwiftKey/final"
enBlogs <- readLines("~/datasciencecoursera/Courses/Capstone/Coursera-SwiftKey/final/en_US/en_US.blogs.txt")
enBlogsLines <- length(enBlogs)
enBlogsChars <- sum(nchar(enBlogs))
enBlogsWords <- unlist(strsplit(enBlogs,split=" "))
enBlogsNumWords <- length(enBlogsWords)
enBlogsWordsUnique <- length(unique(enBlogsWords))
#english twitter
enTwitter <- readLines("~/datasciencecoursera/Courses/Capstone/Coursera-SwiftKey/final/en_US/en_US.twitter.txt")
enTwitterLines <- length(enTwitter)
enTwitterChars <- sum(nchar(enTwitter))
enTwitterWords <- unlist(strsplit(enTwitter,split=" "))
enTwitterNumWords <- length(enTwitterWords)
enTwitterWordsUnique <- length(unique(enTwitterWords))
#english news
enNews <- readLines("~/datasciencecoursera/Courses/Capstone/Coursera-SwiftKey/final/en_US/en_US.news.txt")
enNewsLines <- length(enNews)
enNewsChars <- sum(nchar(enNews))
enNewsWords <- unlist(strsplit(enNews,split=" "))
enNewsNumWords <- length(enNewsWords)
enNewsWordsUnique <- length(unique(enNewsWords))
plot(c(enBlogsWordsUnique, enTwitterWordsUnique, enNewsWordsUnique))
plot(as.data.frame(c(enBlogsWordsUnique, enTwitterWordsUnique, enNewsWordsUnique)))
a <- as.data.frame(c(enBlogsWordsUnique, enTwitterWordsUnique, enNewsWordsUnique))
a
a <- as.numeric(c(enBlogsWordsUnique, enTwitterWordsUnique, enNewsWordsUnique))
a
plot(a)
a <- as.matrix(c(enBlogsWordsUnique, enTwitterWordsUnique, enNewsWordsUnique))
a
plot(a)
dataSummary <- data.frame(Data = character(), Line_Count = integer(), Word_Count = integer(),
Unique_Words = integer(), Total_Characters = integer())
dataSummary[1,1] <- c("Blogs")
dataSummary[1,2] <- enBlogsLines
dataSummary[1,3] <- enBlogsNumWords
dataSummary[1,4] <- enBlogsWordsUnique
dataSummary[1,5] <- enBlogsChars
dataSummary[2,1] <- c("Twitter")
dataSummary[2,2] <- enTwitterLines
dataSummary[2,3] <- enTwitterNumWords
dataSummary[2,4] <- enTwitterWordsUnique
dataSummary[2,5] <- enTwitterChars
dataSummary[3,1] <- c("News")
dataSummary[3,2] <- enNewsLines
dataSummary[3,3] <- enNewsNumWords
dataSummary[3,4] <- enNewsWordsUnique
dataSummary[3,5] <- enNewsChars
dataSummary
dataSummary <- data.frame(Data = character(1), Line_Count = integer(), Word_Count = integer(),
Unique_Words = integer(), Total_Characters = integer())
dataSummary <- data.frame(Data = character(0), Line_Count = integer(), Word_Count = integer(),
Unique_Words = integer(), Total_Characters = integer())
dataSummary[1,1] <- c("Blogs")
dataSummary[1,2] <- enBlogsLines
dataSummary[1,3] <- enBlogsNumWords
dataSummary[1,4] <- enBlogsWordsUnique
dataSummary[1,5] <- enBlogsChars
dataSummary[2,1] <- c("Twitter")
dataSummary[2,2] <- enTwitterLines
dataSummary[2,3] <- enTwitterNumWords
dataSummary[2,4] <- enTwitterWordsUnique
dataSummary[2,5] <- enTwitterChars
dataSummary[3,1] <- c("News")
dataSummary[3,2] <- enNewsLines
dataSummary[3,3] <- enNewsNumWords
dataSummary[3,4] <- enNewsWordsUnique
dataSummary[3,5] <- enNewsChars
dataSummary
dataSummary[1,1]
dataSummary <- data.frame(Data = character(), Line_Count = integer(), Word_Count = integer(),
Unique_Words = integer(), Total_Characters = integer())
dataSummary[1,1] <- c("Blogs")
dataSummary[1,2] <- enBlogsLines
dataSummary[1,3] <- enBlogsNumWords
dataSummary[1,4] <- enBlogsWordsUnique
dataSummary[1,5] <- enBlogsChars
dataSummary[2,1] <- c("Twitter")
dataSummary[2,2] <- enTwitterLines
dataSummary[2,3] <- enTwitterNumWords
dataSummary[2,4] <- enTwitterWordsUnique
dataSummary[2,5] <- enTwitterChars
dataSummary[3,1] <- c("News")
dataSummary[3,2] <- enNewsLines
dataSummary[3,3] <- enNewsNumWords
dataSummary[3,4] <- enNewsWordsUnique
dataSummary[3,5] <- enNewsChars
dataSummary
dataSummary <- data.frame(Data = character(), Line_Count = integer(), Word_Count = integer(),
Unique_Words = integer(), Total_Characters = integer())
dataSummary[1,1] <- "Blogs"
dataSummary[1,2] <- enBlogsLines
dataSummary[1,3] <- enBlogsNumWords
dataSummary[1,4] <- enBlogsWordsUnique
dataSummary[1,5] <- enBlogsChars
dataSummary[2,1] <- c("Twitter")
dataSummary[2,2] <- enTwitterLines
dataSummary[2,3] <- enTwitterNumWords
dataSummary[2,4] <- enTwitterWordsUnique
dataSummary[2,5] <- enTwitterChars
dataSummary[3,1] <- c("News")
dataSummary[3,2] <- enNewsLines
dataSummary[3,3] <- enNewsNumWords
dataSummary[3,4] <- enNewsWordsUnique
dataSummary[3,5] <- enNewsChars
dataSummary
dataSummary <- data.frame(Data = character(), Line_Count = integer(), Word_Count = integer(),
Unique_Words = integer(), Total_Characters = integer(),
stringsAsFactors=FALSE)
dataSummary[1,1] <- "Blogs"
dataSummary[1,2] <- enBlogsLines
dataSummary[1,3] <- enBlogsNumWords
dataSummary[1,4] <- enBlogsWordsUnique
dataSummary[1,5] <- enBlogsChars
dataSummary[2,1] <- c("Twitter")
dataSummary[2,2] <- enTwitterLines
dataSummary[2,3] <- enTwitterNumWords
dataSummary[2,4] <- enTwitterWordsUnique
dataSummary[2,5] <- enTwitterChars
dataSummary[3,1] <- c("News")
dataSummary[3,2] <- enNewsLines
dataSummary[3,3] <- enNewsNumWords
dataSummary[3,4] <- enNewsWordsUnique
dataSummary[3,5] <- enNewsChars
dataSummary
grid.table(dataSummary)
?grid.tabl
?grid.table
install.packages("grid.extra")
library(grid.extra)
install.packages(gridExtra)
install.packages("gridExtra")
library(gridExtra)
library(gridExtra)
grid.table(dataSummary)
enBlogsSubset <- sample(enBlogs, size=enBlogsLines*.01, replace=FALSE) #8992 lines
enTwitterSubset <- sample(enTwitter, size=enTwitterLines*.01, replace=FALSE) #23601 lines
enNewsSubset <- sample(enNews, size=enNewsLines*.01, replace=FALSE) #10102 lines
enSubset <- c(enBlogsSubset, enTwitterSubset, enNewsSubset) #for dfm
en.analyze.dfm <- dfm(enSubset, ignoredFeatures = stopwords("english"), stem = TRUE)
library(quanteda)
enSubset <- c(enBlogsSubset, enTwitterSubset, enNewsSubset) #for dfm
en.analyze.dfm <- dfm(enSubset, ignoredFeatures = stopwords("english"), stem = TRUE)
#25 most frequent words after stemming sans stop words
en.analyze.dfm.freqs <- colSums(as.matrix(en.analyze.dfm))
freq.analyze <- sort(en.analyze.dfm.freqs, decreasing=TRUE)
barplot(freq.analyze[1:25], main="25 Most Frequent Words", xlab="Words", ylab="Occurances", col='green',
las=2, cex.lab=.75, yaxt="n", xaxt="n")
xlabel.analyze <- rownames((as.data.frame(head(freq.analyze,25))))
axis(1, cex.axis=.5, labels=xlabel.analyze, at=1:25, las=2)
axis(2, cex.axis=.5, las=2)
top100words.analyze <- as.data.frame(head(freq.analyze, 100))
wordcloud(rownames(top100words.analyze), top100words.analyze[,1], scale=c(2,.1),
colors=brewer.pal(8, "Dark2"))
library(wordcloud)
top100words.analyze <- as.data.frame(head(freq.analyze, 100))
wordcloud(rownames(top100words.analyze), top100words.analyze[,1], scale=c(2,.1),
colors=brewer.pal(8, "Dark2"))
?wordcloud
?df
?dfm
en.analyze.dfm.unigrams <- dfm(enSubset, ignoredFeatures = stopwords("english"),
stem = TRUE, ngrams = 1, verbose = FALSE)
system.time(en.analyze.dfm.uni.freq <- colSums(as.matrix(en.analyze.dfm.unigrams)))
#user 15.017, system 44.339, elapsed 82.617
uni.freq <- sort(en.analyze.dfm.uni.freq, decreasing=TRUE)
uni.freq.prune <- as.numeric()
for (i in 1:length(uni.freq)) {
if (uni.freq[i] > 10) {
uni.freq.prune <- c(uni.freq.prune, uni.freq[i]) }
}
plot(uni.freq.prune)
?dfm
?as.matrix
class(en.analyze.dfm)
class(en.analyze.dfm.freqs)
class(uni.freq.prune)
barplot(uni.freq.prune[1:25], main="25 Most Frequent Unigrams", xlab="Unigrams", ylab="Occurrences", col='green',
las=2, cex.lab=.75, yaxt="n", xaxt="n")
xlabel.unigram.analyze <- rownames((as.data.frame(head(uni.freq.prune,25))))
axis(1, cex.axis=.5, labels=xlabel.analyze, at=1:25, las=2)
axis(2, cex.axis=.5, las=2)
barplot(uni.freq.prune[1:25], main="25 Most Frequent Unigrams", xlab="Unigrams", ylab="Occurrences", col='green',
las=2, cex.lab=.75, yaxt="n", xaxt="n")
xlabel.unigram.analyze <- rownames((as.data.frame(head(uni.freq.prune,25))))
axis(1, cex.axis=.5, labels=xlabel.unigram.analyze, at=1:25, las=2)
axis(2, cex.axis=.5, las=2)
xlabel.unigram.analyze
uni.freq.prune
barplot(uni.freq.prune[1:25], main="25 Most Frequent Unigrams", xlab="Unigrams", ylab="Occurrences", col='blue',
las=2, cex.lab=.75, yaxt="n", xaxt="n")
xlabel.unigram.analyze <- rownames((as.data.frame(head(uni.freq.prune,25))))
axis(1, cex.axis=.5, labels=xlabel.unigram.analyze, at=1:25, las=2)
axis(2, cex.axis=.5, las=2)
en.analyze.dfm.2grams <- dfm(enSubset, ignoredFeatures = stopwords("english"), stem = TRUE, ngrams = 2, verbose = FALSE)
en.analyze.dfm.two.freq <- colSums(as.matrix(en.analyze.dfm.2grams))
two.freq <- sort(en.analyze.dfm.two.freq, decreasing=TRUE)
two.freq.prune <- as.numeric()
for (i in 1:length(two.freq)) {
if (two.freq[i] > 10) {
two.freq.prune <- c(two.freq.prune, two.freq[i]) }
}
barplot(two.freq.prune[1:25], main="25 Most Frequent Bigrams", xlab="Bigrams", ylab="Occurrences", col='orange',
las=2, cex.lab=.75, yaxt="n", xaxt="n")
xlabel.2gram.analyze <- rownames((as.data.frame(head(two.freq.prune,25))))
axis(1, cex.axis=.5, labels=xlabel.twogram.analyze, at=1:25, las=2)
axis(2, cex.axis=.5, las=2)
barplot(two.freq.prune[1:25], main="25 Most Frequent Bigrams", xlab="Bigrams", ylab="Occurrences", col='orange',
las=2, cex.lab=.75, yaxt="n", xaxt="n")
xlabel.twogram.analyze <- rownames((as.data.frame(head(two.freq.prune,25))))
axis(1, cex.axis=.5, labels=xlabel.twogram.analyze, at=1:25, las=2)
axis(2, cex.axis=.5, las=2)
en.analyze.dfm.bigrams <- dfm(enSubset, ignoredFeatures = stopwords("english"), stem = TRUE, ngrams = 2, verbose = FALSE)
en.analyze.dfm.bi.freq <- colSums(as.matrix(en.analyze.dfm.bigrams))
bi.freq <- sort(en.analyze.dfm.bi.freq, decreasing=TRUE)
bi.freq.prune <- as.numeric()
for (i in 1:length(bi.freq)) {
if (bi.freq[i] > 10) {
bi.freq.prune <- c(bi.freq.prune, bi.freq[i]) }
}
barplot(bi.freq.prune[1:25], main="25 Most Frequent Bigrams", xlab="Bigrams", ylab="Occurrences", col='orange',
las=2, cex.lab=.75, yaxt="n", xaxt="n")
xlabel.bigram.analyze <- rownames((as.data.frame(head(bi.freq.prune,25))))
axis(1, cex.axis=.5, labels=xlabel.bigram.analyze, at=1:25, las=2)
axis(2, cex.axis=.5, las=2)
barplot(uni.freq.prune[1:25], main="25 Most Frequent Unigrams", xlab="Unigrams", ylab="Occurrences", col='red',
las=2, cex.lab=.75, yaxt="n", xaxt="n")
xlabel.unigram.analyze <- rownames((as.data.frame(head(uni.freq.prune,25))))
axis(1, cex.axis=.5, labels=xlabel.unigram.analyze, at=1:25, las=2)
axis(2, cex.axis=.5, las=2)
barplot(bi.freq.prune[1:25], main="25 Most Frequent Bigrams", xlab="Bigrams", ylab="Occurrences", col='orange',
las=2, cex.lab=.75, yaxt="n", xaxt="n")
xlabel.bigram.analyze <- rownames((as.data.frame(head(bi.freq.prune,25))))
axis(1, cex.axis=.5, labels=xlabel.bigram.analyze, at=1:25, las=2)
axis(2, cex.axis=.5, las=2)
bi.freq.prune
uni.freq.prune
en.analyze.dfm.bi.freq
en.analyze.dfm.bigrams
en.analyze.dfm.bi.freq <- colSums(as.matrix(en.analyze.dfm.bigrams))
en.analyze.dfm.bigrams <- dfm(enSubset, ignoredFeatures = stopwords("english"), stem = TRUE, ngrams = 2, verbose = FALSE)
en.analyze.dfm.bi.freq <- colSums(en.analyze.dfm.bigrams)
bi.freq <- sort(en.analyze.dfm.bi.freq, decreasing=TRUE)
bi.freq.prune <- as.numeric()
for (i in 1:length(bi.freq)) {
if (bi.freq[i] > 10) {
bi.freq.prune <- c(bi.freq.prune, bi.freq[i]) }
}
barplot(bi.freq.prune[1:25], main="25 Most Frequent Bigrams", xlab="Bigrams", ylab="Occurrences", col='orange',
las=2, cex.lab=.75, yaxt="n", xaxt="n")
xlabel.bigram.analyze <- rownames((as.data.frame(head(bi.freq.prune,25))))
axis(1, cex.axis=.5, labels=xlabel.bigram.analyze, at=1:25, las=2)
axis(2, cex.axis=.5, las=2)
en.analyze.dfm.unigrams <- dfm(enSubset, ignoredFeatures = stopwords("english"), stem = TRUE, ngrams = 1, verbose = FALSE)
en.analyze.dfm.uni.freq <- colSums(en.analyze.dfm.unigrams)
uni.freq <- sort(en.analyze.dfm.uni.freq, decreasing=TRUE)
uni.freq.prune <- as.numeric()
for (i in 1:length(uni.freq)) {
if (uni.freq[i] > 10) {
uni.freq.prune <- c(uni.freq.prune, uni.freq[i]) }
}
barplot(uni.freq.prune[1:25], main="25 Most Frequent Unigrams", xlab="Unigrams", ylab="Occurrences", col='red',
las=2, cex.lab=.75, yaxt="n", xaxt="n")
xlabel.unigram.analyze <- rownames((as.data.frame(head(uni.freq.prune,25))))
axis(1, cex.axis=.5, labels=xlabel.unigram.analyze, at=1:25, las=2)
axis(2, cex.axis=.5, las=2)
en.analyze.dfm.trigrams <- dfm(enSubset, ignoredFeatures = stopwords("english"), stem = TRUE, ngrams = 3, verbose = FALSE)
en.analyze.dfm.tri.freq <- colSums(en.analyze.dfm.trigrams)
tri.freq <- sort(en.analyze.dfm.tri.freq, decreasing=TRUE)
tri.freq.prune <- as.numeric()
for (i in 1:length(tri.freq)) {
if (tri.freq[i] > 10) {
tri.freq.prune <- c(tri.freq.prune, tri.freq[i]) }
}
barplot(tri.freq.prune[1:25], main="25 Most Frequent Trigrams", xlab="Bigrams", ylab="Occurrences", col='yellow',
las=2, cex.lab=.75, yaxt="n", xaxt="n")
xlabel.trigram.analyze <- rownames((as.data.frame(head(tri.freq.prune,25))))
axis(1, cex.axis=.5, labels=xlabel.trigram.analyze, at=1:25, las=2)
axis(2, cex.axis=.5, las=2)
en.analyze.dfm.quadgrams <- dfm(enSubset, ignoredFeatures = stopwords("english"), stem = TRUE, ngrams = 4, verbose = FALSE)
en.analyze.dfm.quad.freq <- colSums(en.analyze.dfm.quadgrams)
quad.freq <- sort(en.analyze.dfm.quad.freq, decreasing=TRUE)
quad.freq.prune <- as.numeric()
for (i in 1:length(quad.freq)) {
if (quad.freq[i] > 10) {
quad.freq.prune <- c(quad.freq.prune, quad.freq[i]) }
}
barplot(quad.freq.prune[1:25], main="25 Most Frequent Bigrams", xlab="Quadgrams", ylab="Occurrences", col='orange',
las=2, cex.lab=.75, yaxt="n", xaxt="n")
xlabel.quadgram.analyze <- rownames((as.data.frame(head(quad.freq.prune,25))))
axis(1, cex.axis=.5, labels=xlabel.quadgram.analyze, at=1:25, las=2)
axis(2, cex.axis=.5, las=2)
barplot(quad.freq.prune[1:25], main="25 Most Frequent Bigrams", xlab="Quadgrams", ylab="Occurrences", col='#CC5500',
las=2, cex.lab=.75, yaxt="n", xaxt="n")
xlabel.quadgram.analyze <- rownames((as.data.frame(head(quad.freq.prune,25))))
axis(1, cex.axis=.5, labels=xlabel.quadgram.analyze, at=1:25, las=2)
axis(2, cex.axis=.5, las=2)
en.analyze.dfm.quadgrams <- dfm(enSubset, ignoredFeatures = stopwords("english"), stem = TRUE, ngrams = 4, verbose = FALSE)
en.analyze.dfm.quad.freq <- colSums(en.analyze.dfm.quadgrams)
quad.freq <- sort(en.analyze.dfm.quad.freq, decreasing=TRUE)
quad.freq.prune <- as.numeric()
for (i in 1:length(quad.freq)) {
if (quad.freq[i] > 10) {
quad.freq.prune <- c(quad.freq.prune, quad.freq[i]) }
}
barplot(quad.freq.prune[1:25], main="25 Most Frequent Quadgrams", xlab="Quadgrams", ylab="Occurrences", col='#500000',
las=2, cex.lab=.75, yaxt="n", xaxt="n")
xlabel.quadgram.analyze <- rownames((as.data.frame(head(quad.freq.prune,25))))
axis(1, cex.axis=.5, labels=xlabel.quadgram.analyze, at=1:25, las=2)
axis(2, cex.axis=.5, las=2)
en.analyze.dfm.pentagrams <- dfm(enSubset, ignoredFeatures = stopwords("english"), stem = TRUE, ngrams = 5, verbose = FALSE)
en.analyze.dfm.penta.freq <- colSums(en.analyze.dfm.pentagrams)
penta.freq <- sort(en.analyze.dfm.penta.freq, decreasing=TRUE)
penta.freq.prune <- as.numeric()
for (i in 1:length(penta.freq)) {
if (penta.freq[i] > 10) {
penta.freq.prune <- c(penta.freq.prune, penta.freq[i]) }
}
barplot(penta.freq.prune[1:25], main="25 Most Frequent Pentagrams", xlab="Pentagrams", ylab="Occurrences", col='#500000',
las=2, cex.lab=.75, yaxt="n", xaxt="n")
xlabel.pentagram.analyze <- rownames((as.data.frame(head(penta.freq.prune,25))))
axis(1, cex.axis=.5, labels=xlabel.pentagram.analyze, at=1:25, las=2)
axis(2, cex.axis=.5, las=2)
en.analyze.dfm.hectagrams <- dfm(enSubset, ignoredFeatures = stopwords("english"), stem = TRUE, ngrams = 5, verbose = FALSE)
en.analyze.dfm.hecta.freq <- colSums(en.analyze.dfm.hectagrams)
hecta.freq <- sort(en.analyze.dfm.hecta.freq, decreasing=TRUE)
hecta.freq.prune <- as.numeric()
for (i in 1:length(hecta.freq)) {
if (hecta.freq[i] > 10) {
hecta.freq.prune <- c(hecta.freq.prune, hecta.freq[i]) }
}
barplot(hecta.freq.prune[1:25], main="25 Most Frequent Hectagrams", xlab="Hectagrams", ylab="Occurrences", col='#500000',
las=2, cex.lab=.75, yaxt="n", xaxt="n")
xlabel.hectagram.analyze <- rownames((as.data.frame(head(hecta.freq.prune,25))))
axis(1, cex.axis=.5, labels=xlabel.hectagram.analyze, at=1:25, las=2)
axis(2, cex.axis=.5, las=2)
hecta.freq.prune[1:25]
en.analyze.dfm.hectagrams <- dfm(enSubset, ignoredFeatures = stopwords("english"), stem = TRUE, ngrams = 6, verbose = FALSE)
en.analyze.dfm.hecta.freq <- colSums(en.analyze.dfm.hectagrams)
hecta.freq <- sort(en.analyze.dfm.hecta.freq, decreasing=TRUE)
hecta.freq.prune <- as.numeric()
for (i in 1:length(hecta.freq)) {
if (hecta.freq[i] > 10) {
hecta.freq.prune <- c(hecta.freq.prune, hecta.freq[i]) }
}
barplot(hecta.freq.prune[1:25], main="25 Most Frequent Hectagrams", xlab="Hectagrams", ylab="Occurrences", col='#500000',
las=2, cex.lab=.75, yaxt="n", xaxt="n")
xlabel.hectagram.analyze <- rownames((as.data.frame(head(hecta.freq.prune,25))))
axis(1, cex.axis=.5, labels=xlabel.hectagram.analyze, at=1:25, las=2)
axis(2, cex.axis=.5, las=2)
xlabel.hectagram.analyze
length(xlabel.hectagram.analyze[6])
xlabel.hectagram.analyze[6]
?numwords
?numWords
?grep
grep("_", xlabel.hectagram.analyze[6] )
strsplit(xlabel.hectagram.analyze[6], split="_")
length(xlabel.hectagram.analyze[6])
unlist(strsplit(xlabel.hectagram.analyze[6], split="_"))
length(unlist(strsplit(xlabel.hectagram.analyze[6], split="_")))
length(xlabel.hectagram.analyze)
require(fixNgram.R)
fixNgram.R(hecta.freq.prune,6)
hecta.freq.prune
head(hecta.freq.prune)
length(hecta.freq.prune)
fixNgram <- function(x, n=integer()) {
a <- as.numeric()
for (i in 1:length(x)) {
if(length(unlist(strsplit(x[i], split="_"))) > (n-1){
a <- c(a,x[i])
}
}
x <- a
x
}
fixNgram(hecta.freq.prune,6)
fixNgram <- function(x, n=integer()) {
a <- as.numeric()
for (i in 1:length(x)) {
if(length(unlist(strsplit(x[i], split="_"))) > (n-1){
a <- c(a,x[i])
}
}
x <- a
x
}
hecta.freq.prune.fix <- fixNgram(hecta.freq.prune,6)
hecta.freq.prune.fix <- fixNgram(hecta.freq.prune,6)
source('~/.active-rstudio-document')
?strsplit
source('~/datasciencecoursera/Courses/R Programming/ProgrammingAssignment2/cachematrix.R')
source('~/datasciencecoursera/Courses/Capstone/capstoneproject/fixNgram.R')
hecta.freq.prune.fix <- fixNgram(hecta.freq.prune,6)
source('~/datasciencecoursera/Courses/Capstone/capstoneproject/fixNgram.R')
hecta.freq.prune.fix <- fixNgram(hecta.freq.prune,6)
hecta.freq.prune.fix
hecta.freq.prune
fixNgram(hecta.freq.prune,6)
c <- hecta.freq.prune
unlist(strsplit(c[1], split="_"))
strsplit(c[1], split="_")
class(enTwitter)
class(hecta.freq.prune[1])
fixNgram(xlabel.hectagram.analyze, 6)
fixNgram(xlabel.hectagram.analyze, 6)
x
class(unlist(strsplit(x[i], split="_")))
class(xlabel.hectagram.analyze)
source('~/datasciencecoursera/Courses/Capstone/capstoneproject/fixNgram.R')
fixNgram(xlabel.hectagram.analyze, 6)
x
xlabel.hectagram.analyze[1]
lengh(unlist(strsplit(xlabel.hectagram.analyze[3])))
length(unlist(strsplit(xlabel.hectagram.analyze[3])))
length(unlist(strsplit(xlabel.hectagram.analyze[3], split="_")))
class(length(unlist(strsplit(xlabel.hectagram.analyze[3], split="_"))))
source('~/datasciencecoursera/Courses/Capstone/capstoneproject/fixNgram.R')
fixNgram(xlabel.hectagram.analyze, 6)
aa <- fixNgram(xlabel.hectagram.analyze)
aa <- fixNgram(xlabel.hectagram.analyze, 6)
aa
aa <- fixNgram(xlabel.pentagram.analyze, 5)
aa
xlabel.pentagram <- rownames((as.data.frame(penta.freq.prune)))
xlabel.pentagram
xlabel.pentagram <- rownames((as.data.frame(penta.freq.prune)))
pentagram.char <- fixNgram(xlabel.pentagram, 5)
pentagram.char
source('~/datasciencecoursera/Courses/Capstone/capstoneproject/fixNgram.R')
