set.seed(3443)
IL <- grep("^IL", colnames(training), values=TRUE)
newpredictors <- predictors[,IL]
newData=data.frame(diagnosis, newpredictors)
in2Train <- createDataPartition(newData$diagnosis, p=3/4)[[1]]
newTrain <- newData[in2Train,]
newTest <- newData[-in2Train,]
## MODELS
## As they are
modelFit <- train(diagnosis ~ ., method="glm", data=newTrain)
PmodelFit <- predict(modelFit, newTest)
CMmodelFit <- confusionMatrix(newTest$diagnosis, PmodelFit)
## PCA with 80% variance explained
set.seed(3443)
modelFit80 <- train(diagnosis ~ ., method="glm", preProcess="pca", trControl = trainControl(preProcOptions = list(thresh = 0.8)), data=newTrain)
PmodelFit80 <- predict(modelFit80, newTest)
CMmodelFit80 <- confusionMatrix(newTest$diagnosis, PmodelFit80)
print(CMmodelFit)
print(CMmodelFit80)
library(caret)
library(AppliedPredictiveModeling)
set.seed(3433)
data(AlzheimerDisease)
adData = data.frame(diagnosis,predictors)
inTrain = createDataPartition(adData$diagnosis, p = 3/4)[[1]]
training = adData[ inTrain,]
testing = adData[-inTrain,]
set.seed(3443)
IL <- grep("^IL", colnames(training), value=TRUE)
newpredictors <- predictors[,IL]
newData=data.frame(diagnosis, newpredictors)
in2Train <- createDataPartition(newData$diagnosis, p=3/4)[[1]]
newTrain <- newData[in2Train,]
newTest <- newData[-in2Train,]
## MODELS
## As they are
modelFit <- train(diagnosis ~ ., method="glm", data=newTrain)
PmodelFit <- predict(modelFit, newTest)
CMmodelFit <- confusionMatrix(newTest$diagnosis, PmodelFit)
## PCA with 80% variance explained
set.seed(3443)
modelFit80 <- train(diagnosis ~ ., method="glm", preProcess="pca", trControl = trainControl(preProcOptions = list(thresh = 0.8)), data=newTrain)
PmodelFit80 <- predict(modelFit80, newTest)
CMmodelFit80 <- confusionMatrix(newTest$diagnosis, PmodelFit80)
print(CMmodelFit)
print(CMmodelFit80)
library(caret)
library(AppliedPredictiveModeling)
set.seed(3433)
data(AlzheimerDisease)
adData = data.frame(diagnosis,predictors)
inTrain = createDataPartition(adData$diagnosis, p = 3/4)[[1]]
training = adData[ inTrain,]
testing = adData[-inTrain,]
set.seed(3433)
IL <- grep("^IL", colnames(training), value=TRUE)
newpredictors <- predictors[,IL]
newData=data.frame(diagnosis, newpredictors)
in2Train <- createDataPartition(newData$diagnosis, p=3/4)[[1]]
newTrain <- newData[in2Train,]
newTest <- newData[-in2Train,]
## MODELS
## As they are
modelFit <- train(diagnosis ~ ., method="glm", data=newTrain)
PmodelFit <- predict(modelFit, newTest)
CMmodelFit <- confusionMatrix(newTest$diagnosis, PmodelFit)
## PCA with 80% variance explained
set.seed(3433)
modelFit80 <- train(diagnosis ~ ., method="glm", preProcess="pca", trControl = trainControl(preProcOptions = list(thresh = 0.8)), data=newTrain)
PmodelFit80 <- predict(modelFit80, newTest)
CMmodelFit80 <- confusionMatrix(newTest$diagnosis, PmodelFit80)
print(CMmodelFit)
print(CMmodelFit80)
library(caret)
library(AppliedPredictiveModeling)
set.seed(3433)
data(AlzheimerDisease)
adData = data.frame(diagnosis,predictors)
inTrain = createDataPartition(adData$diagnosis, p = 3/4)[[1]]
training = adData[ inTrain,]
testing = adData[-inTrain,]
IL <- grep("^IL", colnames(training), value=TRUE)
newpredictors <- predictors[,IL]
newData=data.frame(diagnosis, newpredictors)
in2Train <- createDataPartition(newData$diagnosis, p=3/4)[[1]]
newTrain <- newData[in2Train,]
newTest <- newData[-in2Train,]
## MODELS
## As they are
modelFit <- train(diagnosis ~ ., method="glm", data=newTrain)
PmodelFit <- predict(modelFit, newTest)
CMmodelFit <- confusionMatrix(newTest$diagnosis, PmodelFit)
## PCA with 80% variance explained
modelFit80 <- train(diagnosis ~ ., method="glm", preProcess="pca", trControl = trainControl(preProcOptions = list(thresh = 0.8)), data=newTrain)
PmodelFit80 <- predict(modelFit80, newTest)
CMmodelFit80 <- confusionMatrix(newTest$diagnosis, PmodelFit80)
print(CMmodelFit)
print(CMmodelFit80)
library(AppliedPredictiveModeling)
data(segmentationOriginal)
library(caret)
View(segmentationOriginal)
colname(segmentationOriginal)
colnames(segmentationOriginal)
training <- segmentationOriginal[segmentationOriginal$Case=="Train",]
View(training)
View(segmentationOriginal)
testing <- segmentationOriginal[segmentationOriginal$Case == "Test",]
set.seed(125)
modFit <- (Case ~ ., method = "rpart", segmentationOriginal)
set.seed(125)
modFit <- (Case ~ ., method = "rpart", data=training)
set.seed(125)
modFit <- (Case ~ . , method = "rpart", data=training)
set.seed(125)
modFit <- train(Case ~ ., method = "rpart", training)
set.seed(125)
modFit <- train(Case ~ ., method = "rpart", data=training)
set.seed(125)
modFit <- train(Case ~ ., data = training, method = "rpart")
View(testing)
View(training)
is.na(training)
table(is.na(training))
library(rpart)
set.seed(125)
modFit <- train(Case ~ ., data = training, method = "rpart")
training$Case
train <- subset(segmentationOriginal, case=="Train")
test <- subset(segmentationOriginal, case=="Test")
train <- subset(segmentationOriginal, Case=="Train")
test <- subset(segmentationOriginal, Case=="Test")
is.identical(training, train)
identical(training, train)
identical(testing, test)
set.seed(125)
modFit <- train(Case ~ ., data = train, method = "rpart")
library(AppliedPredictiveModeling)
data(segmentationOriginal)
library(caret)
library(rpart)
training <- segmentationOriginal[segmentationOriginal$Case == "Train",]
testing <- segmentationOriginal[segmentationOriginal$Case == "Test",]
set.seed(125)
modFit <- train(Case ~ ., data = training, method = "rpart")
set.seed(125)
modFit <- train(Case ~ ., data = training, method = "rpart")
install.packages(rattle)
install.packages("rattle")
library(rattle)
set.seed(125)
modFit <- train(Case ~ ., data = training, method = "rpart")
set.seed(125)
modFit <- train(Case ~ ., data = training, method = "rpart")
library(AppliedPredictiveModeling)
data(segmentationOriginal)
library(caret)
library(rpart)
set.seed(125)
inTrain <- createDataPartition(y = segmentationOriginal$Case, list = FALSE)
train <- subset(segmentationOriginal, Case == "Train")
test <- subset(segmentationOriginal, Case == "Test")
modFit <- train(Class ~ ., data = train, method = "rpart")
library(AppliedPredictiveModeling)
data(segmentationOriginal)
library(caret)
library(rpart)
set.seed(125)
training <- segmentationOriginal[segmentationOriginal$Case == "Train",]
testing <- segmentationOriginal[segmentationOriginal$Case == "Test",]
modFit <- train(Case ~ ., data = training, method = "rpart")
segmentationOriginal
head(segmentationOriginal)
modFit <- train(Class ~ ., data = training, method = "rpart")
library(AppliedPredictiveModeling)
data(segmentationOriginal)
library(caret)
library(rpart)
set.seed(125)
training <- segmentationOriginal[segmentationOriginal$Case == "Train",]
testing <- segmentationOriginal[segmentationOriginal$Case == "Test",]
modFit <- train(Class ~ ., data = training, method = "rpart")
library(AppliedPredictiveModeling)
data(segmentationOriginal)
library(caret)
library(rpart)
training <- segmentationOriginal[segmentationOriginal$Case == "Train",]
testing <- segmentationOriginal[segmentationOriginal$Case == "Test",]
set.seed(125)
modFit <- train(Class ~ ., training, method = "rpart")
modFinal <- train(Class ~ ., training, method="rpart", TotalIntench2=23,000, FiberWidthCh1=10, PerimStatusCh1=2)
modfit$FinalModel
modFit$FinalModel
modFit$finalModel
library(pgmm)
data(olive)
olive = olive[,-1]
library(pgmm)
data(olive)
olive = olive[,-1]
install.packages("pgmm")
library(pgmm)
data(olive)
olive = olive[,-1]
modelFit <- train(Area ~ ., olive, method="rpart")
fancyRpartPlot(modFit$finalodel)
fancyRpartPlot(modFit$finalModel)
library(rattle)
fancyRpartPlot(modFit$finalModel)
install.packages("rattle")
install.packages("rattle")
library(rattle)
fancyRpartPlot(modFit$finalModel)
library(caret)
fancyRpartPlot(modFit$finalModel)
library("rpart.plot")
install.packages("rpart.plot")
library(rpart.plot)
fancyRpartPlot(modFit$finalModel)
newdata = as.data.frame(t(colMeans(olive)))
newModel <- predict(modelFit, newdata)
newModel
colnames(olive)
class(olive$Area)
modelFit$finalModel
library(ElemStatLearn)
data(SAheart)
set.seed(8484)
train = sample(1:dim(SAheart)[1],size=dim(SAheart)[1]/2,replace=F)
trainSA = SAheart[train,]
testSA = SAheart[-train,]
install.packages("ElemStatLearn")
library(ElemStatLearn)
library(ElemStatLearn)
data(SAheart)
set.seed(8484)
train = sample(1:dim(SAheart)[1],size=dim(SAheart)[1]/2,replace=F)
trainSA = SAheart[train,]
testSA = SAheart[-train,]
colnames(SAheart)
set.seed(13234)
modelFit <- train(chd~age+alcohol+obesity+tobacco+typea+ldl, trainSA, method="glm", family="binomial")
prediction <- predict(modelFit, testSA)
missClass = function(values,prediction){sum(((prediction > 0.5)*1) != values)/length(values)}
missClass
missClass(testSA)
missClass(prediction)
missClass(trainSA$chd, predict(modelFit, trainSA))
missTrain <- missClass(trainSA$chd, predict(modelFit, trainSA))
missTest <- missClass(trainSA$chd, predict(modelFit, testSA))
missTrain
missTest
missTrain <- missClass(trainSA$chd, predict(modelFit, trainSA))
missTest <- missClass(testSA$chd, predict(modelFit, testSA))
missTrain
missTest
library(ElemStatLearn)
data(vowel.train)
data(vowel.test)
colnames(ElemStatLearn)
head(ElemStatLearn)
ElemStatLearn
library(ElemStatLearn)
data(vowel.train)
data(vowel.test)
vowel.train
class(vowel.train)
colnames(vowel.train)
class(vowel.train$y)
vowel.train$y <- as.factor(vowel.train$y)
vowel.test$y <- as.factor(vowel.test$y)
class(vowel.test$y)
set.seed(33833)
modelFit <- train(y~., vowel.train, method="rf")
order(varImp(modelFit))
set.seed(33833)
modelFit <- train(y~., vowel.train, method="rf", prox=TRUE)
order(varImp(modelFit))
modelFit
varImp(modelFit)
order(varImp(modelFit))
class(modelFit)
a <- randomForest(y ~ ., data = vowel.train, importance = FALSE)
varImp(a)
order(varImp(a))
vowel.train$y <- as.factor(vowel.train$y)
vowel.test$y <- as.factor(vowel.test$y)
set.seed(33833)
modelFit <- train(y~., vowel.train, method="rf", prox=TRUE)
varImp(modelFit)
prediction <- predict(vowel.test, modelFit)
prediction <- predict(modelFit, vowel.test)
varImp(prediction)
prediction
modelFit
prediction
vowel.train$y <- as.factor(vowel.train$y)
vowel.test$y <- as.factor(vowel.test$y)
set.seed(3833)
modelFit <- train(y~., vowel.train, method="rf", prox=TRUE)
varImp(modelFit)
library(ISLR)
install.packages("ISLR")
library(ISLR)
data(Wage)
library(ggplot2)
library(caret)
inBuild <- createDataPartition(y=Wage$wage, p=.7, list=FALSE)
validation <- Wage[-inBuild,]
buildData <- Wage[inBuild]
inTrain <- createDataPartition(y=buildData$wage, p=.7, list=FALSE)
buildData <- Wage[inBuild,]
inTrain <- createDataPartition(y=buildData$wage, p=.7, list=FALSE)
training <- buildData[inTrain,]
testing <- buildData[-inTrain,]
dim(training)
dim(testing)
dim(validatin)
dim(validation)
mod1 <- train(wage ~ ., method="glm", data=training)
mod2 <- train(wage ~., method="rf", data=training, trControl=trainControl(method="cv"), number=3)
pred1 <- predict(mod1, testing)
pred1
pred2 <- predict(mod2, testing)
qplot(pred1, pred2, color=wage, data=testing)
testing$wage
training$wage
preDF <- data.frame(pred1, pred2, wage=testing$wage)
preDF
head(preDF)
exit
a <- c(4, 0, 4, 0, 3, 0, 10, 1, 6, 4, 3, 8, 2, 3, 1, 10, 7, 5, 2, 1, 5, 1, 1, 2)
plot(a)
hist(a)
hist(a, breaks=10)
hist(a, breaks=12)
hist(a, breaks=c(0:10))
hist(a, breaks=c(0:11))
hist(a, breaks=c(0:10))
mean(a)
abline(v = mean(a), col = "blue", lwd = 2)
hist(a, breaks=10, main="Rangers Runs Per Game in April"
)
hist(a, breaks=10, main="Rangers Runs Per Game in April", x.lab="Runs Per Game")
hist(a, breaks=10, main="Rangers Runs Per Game in April", xlab="Runs Per Game")
abline(v = mean(a), col = "blue", lwd = 2)
library(stringi)
library(tm)
library(quanteda)
source('~/datasciencecoursera/Courses/Capstone/capstoneproject/ngramTailFinder.R')
enBlogs <- stri_read_lines("~/datasciencecoursera/Courses/Capstone/Coursera-SwiftKey/final/en_US/en_US.blogs.txt") #user 3.437, system .496, elapsed 3.982
enBlogsSubset <- sample(enBlogs, size=enBlogsLines*.01, replace=FALSE) #8992 lines
enBlogsSubset <- sample(enBlogs, size=length(enBlogs)*.01, replace=FALSE) #8992 lines
trigram <- ngramTailFinder("The guy in front of me just bought a pound of bacon, a bouquet, and a case of", 3)
system.time(enBlogsQuadgrams <- dfm(enBlogsSubset, ngrams = 4, verbose = TRUE))
enBlogsQuadFreq <- colSums(enBlogsQuadgrams)
enBlogsQuadFreq <- sort(enBlogsQuadFreq, decreasing=TRUE)
enBlogsQuadFreq.prune <- as.numeric()
for (i in 1:length(enBlogsQuadFreq)) {
if (enBlogsQuadFreq[i] > 2) {
enBlogsQuadFreq.prune <- c(enBlogsQuadFreq.prune, enBlogsQuadFreq[i]) }
}
enBlogsQuadFreq.prune
enBlogsQuadFreq.prune$
dimnames
str(enBlogsQuadFreq.prune)
class(enBlogsQuadFreq.prune)
length(enBlogsQuadFreq.prune)
a <- enBlogsQuadFreq.prune[1]
a
a <- enBlogsQuadFreq.prune[grepl(c"^",trigram),enBlogsQuadFreq.prune]
a <- enBlogsQuadFreq.prune[grepl(c("^",trigram),enBlogsQuadFreq.prune)]
a
trigram
aa <- c("^",trigram)
aa
aa <- paste(c("^", trigram), collapse="")
aa
a <- enBlogsQuadFreq.prune[grepl(paste(c("^", trigram), collapse=""),enBlogsQuadFreq.prune)]
a
enBlogsQuadFreq.prune
trigram <- "you may want to"
foundQuads <- enBlogsQuadFreq.prune[grepl(paste(c("^", trigram), collapse=""),enBlogsQuadFreq.prune)]
foundQuads
trigram <- "you_may_want"
foundQuads <- enBlogsQuadFreq.prune[grepl(paste(c("^", trigram), collapse=""),enBlogsQuadFreq.prune)]
foundQuads
foundQuads <- enBlogsQuadFreq.prune[grepl(trigram, collapse=""),enBlogsQuadFreq.prune)]
foundQuads <- enBlogsQuadFreq.prune[grepl(trigram, collapse=""),enBlogsQuadFreq.prune]
foundQuads <- enBlogsQuadFreq.prune[grepl(trigram,enBlogsQuadFreq.prune)]
foundQuads
trigram
a<-grepl(trigram,enBlogsQuadFreq.prune)
a
table(a)
a<-grepl("you_may_want",enBlogsQuadFreq.prune)
a
table(a)
a<-grepl("^you_may_want.",enBlogsQuadFreq.prune)
a
table(a)
a<-grepl("^you_may_want(.)",enBlogsQuadFreq.prune)
table(a)
a<-grepl("^you_may_want.*",enBlogsQuadFreq.prune)
table(a)
a<-grepl("you_may_want",enBlogsQuadFreq.prune)
a
table(a)
a<-grepl('you_may_want',enBlogsQuadFreq.prune)
table(a)
a<-grepl("you",enBlogsQuadFreq.prune)
table(a)
a<-grepl(".you.",enBlogsQuadFreq.prune)
table(a)
?grepl
enBlogsQuadFreq.prune
a<-grepl("3",enBlogsQuadFreq.prune)
table(a)
row.names(a)
row.names(enBlogsQuadFreq.prune)
dim(enBlogsQuadFreq.prune)
str(enBlogsQuadFreq.prune)
enBlogsQuadFreq.prune$names
enBlogsQuadFreq.prune[names]
enBlogsQuadFreq.prune[names,]
enBlogsQuadFreq.prune[,names]
enBlogsQuadFreq.prune$attr$names
class(enBlogsQuadFreq.prune)
names(enBlogsQuadFreq.prune)
foundQuads <- enBlogsQuadFreq.prune[grepl(paste(c("^", trigram), collapse=""),names(enBlogsQuadFreq.prune)]
foundQuads <- enBlogsQuadFreq.prune[grepl(paste(c("^", trigram), collapse=""),names(enBlogsQuadFreq.prune))]
foundQuads
trigram <- ngramTailFinder("The guy in front of me just bought a pound of bacon, a bouquet, and a case of", 3)
system.time(enBlogsQuadgrams <- dfm(enBlogsSubset, ngrams = 4, verbose = TRUE))
enBlogsQuadFreq <- colSums(enBlogsQuadgrams)
enBlogsQuadFreq <- sort(enBlogsQuadFreq, decreasing=TRUE)
enBlogsQuadFreq.prune <- as.numeric()
for (i in 1:length(enBlogsQuadFreq)) {
if (enBlogsQuadFreq[i] > 2) {
enBlogsQuadFreq.prune <- c(enBlogsQuadFreq.prune, enBlogsQuadFreq[i]) }
}
foundQuads <- enBlogsQuadFreq.prune[grepl(paste(c("^", trigram), collapse=""),names(enBlogsQuadFreq.prune))]
foundQuads
trigram
library(stringi)
library(tm)
library(quanteda)
source('~/datasciencecoursera/Courses/Capstone/capstoneproject/ngramTailFinder.R')
enBlogs <- stri_read_lines("~/datasciencecoursera/Courses/Capstone/Coursera-SwiftKey/final/en_US/en_US.blogs.txt") #user 3.437, system .496, elapsed 3.982
enBlogsSubset <- sample(enBlogs, size=length(enBlogs)*.25, replace=FALSE)
trigram <- ngramTailFinder("The guy in front of me just bought a pound of bacon, a bouquet, and a case of", 3)
enBlogsQuadgrams <- dfm(enBlogsSubset, ngrams = 4, verbose = TRUE)
enBlogsQuadFreq <- colSums(enBlogsQuadgrams)
enBlogsQuadFreq <- sort(enBlogsQuadFreq, decreasing=TRUE)
enBlogsQuadFreq.prune <- as.numeric()
for (i in 1:length(enBlogsQuadFreq)) {
if (enBlogsQuadFreq[i] > 2) {
enBlogsQuadFreq.prune <- c(enBlogsQuadFreq.prune, enBlogsQuadFreq[i]) }
}
foundQuads <- enBlogsQuadFreq.prune[grepl(paste(c("^", trigram), collapse=""),names(enBlogsQuadFreq.prune))]
for (i in 1:length(enBlogsQuadFreq)) {
if (enBlogsQuadFreq[i] > 2) {
enBlogsQuadFreq.prune <- c(enBlogsQuadFreq.prune, enBlogsQuadFreq[i]) }
}
foundQuads <- enBlogsQuadFreq[grepl(paste(c("^", trigram), collapse=""),names(enBlogsQuadFreq))]
foundQuads
class(enBlogsQuadFreq)
names(enBlogsQuadFreq)
trigram
trigram <- ngramTailFinder("The guy in front of me just bought a pound of bacon, a bouquet, and a case of", 3)
trigram
source('~/datasciencecoursera/Courses/Capstone/capstoneproject/ngramTailFinder.R')
trigram <- ngramTailFinder("The guy in front of me just bought a pound of bacon, a bouquet, and a case of", 3)
trigram
foundQuads <- enBlogsQuadFreq[grepl(paste(c("^", trigram), collapse=""),names(enBlogsQuadFreq))]
foundQuads
devtools::install_github("kbenoit/quanteda”)
devtools::install_github("kbenoit/quanteda”)
enBlogs <- stri_read_lines("~/datasciencecoursera/Courses/Capstone/Coursera-SwiftKey/final/en_US/en_US.blogs.txt") #user 3.437, system .496, elapsed 3.982
set.seed(100)
enBlogsSubset <- sample(enBlogs, size=length(enBlogs)*1, replace=FALSE)
source('~/datasciencecoursera/Courses/Capstone/capstoneproject/ngramTailFinder.R')
source('~/datasciencecoursera/Courses/Capstone/capstoneproject/ngramTailFinder1.R')
library(stringi)
library(tm)
library(quanteda)
setwd("~/datasciencecoursera/Courses/Capstone/capstoneproject")
enBlogs <- stri_read_lines("~/datasciencecoursera/Courses/Capstone/Coursera-SwiftKey/final/en_US/en_US.blogs.txt") #user 3.437, system .496, elapsed 3.982
set.seed(100)
enBlogsSubset <- sample(enBlogs, size=length(enBlogs)*1, replace=FALSE)
enBlogsUnigrams <- dfm(enBlogsSubset, ngrams = 1)
rm(enBlogsUnigrams)
enBlogs <- stri_read_lines("~/datasciencecoursera/Courses/Capstone/Coursera-SwiftKey/final/en_US/en_US.blogs.txt") #user 3.437, system .496, elapsed 3.982
set.seed(100)
enBlogsSubset <- sample(enBlogs, size=length(enBlogs)*.01, replace=FALSE)
enBlogsUnigrams <- dfm(enBlogsSubset, ngrams = 1)
save(enBlogsUnigrams, file="~/datasciencecoursera/Courses/Capstone/capstoneproject/enBlogsUnigrams.RData")
unlink("~/datasciencecoursera/Courses/Capstone/capstoneproject/enBlogsUnigrams.RData")
unlink("~/datasciencecoursera/Courses/Capstone/capstoneproject/.RData")
save(enBlogsUnigrams, file="~/datasciencecoursera/Courses/Capstone/capstoneproject/enBlogsUnigrams.RData")
enBlogsBigrams <- dfm(enBlogsSubset, ngrams = 2)
save(enBlogsBigrams, file="~/datasciencecoursera/Courses/Capstone/capstoneproject/enBlogsBigrams.RData")
unigrams <- load("~/datasciencecoursera/Courses/Capstone/capstoneproject/enBlogsUnigrams.RData")
unigrams
?attach
unigrams <- attach("~/datasciencecoursera/Courses/Capstone/capstoneproject/enBlogsUnigrams.RData")
unigrams
enBlogsGrams <- unigrams
enBlogsGramsFreq <- topfeatures(enBlogsGrams, n = nfeature(enBlogsGrams))
source('~/datasciencecoursera/Courses/Capstone/capstoneproject/NextWordPredictor.R')
NextWordPredictor("You're the reason why I smile everyday. Can you follow me please? It would mean the", 4)
NextWordPredictor("You're the reason why I smile everyday. Can you follow me please? It would mean the", 4, " ")
